{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1eae78d8-7079-42f5-8f42-25896308d1ce",
   "metadata": {},
   "source": [
    "# Working with text data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de12720-b4b5-492d-8287-0fc644cc5962",
   "metadata": {},
   "source": [
    "Here we will look into:\n",
    "- Preparing text for LLM model training.\n",
    "- Splitting text into word and subword tokens.\n",
    "- Byte pair encoding.\n",
    "- Sampling training examples using a sliding window approach.\n",
    "- Converting tokens into vectors.\n",
    "\n",
    "For the purposes of learning we will work with the text of short story by Edith Wharton called \"The Verdict.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2882fea2-aff9-4e55-8e77-d6cfda0a1363",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4ddeb3e-bd87-493a-b9ea-016da894e654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./data/the-verdict.txt', <http.client.HTTPMessage at 0x10bca0430>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
    "       \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
    "       \"the-verdict.txt\")\n",
    "urllib.request.urlretrieve(url, './data/the-verdict.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bf39fcb9-e027-4424-8d78-ac6d9b2f747e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num characters: 20,479\n",
      "Num words: 3,552\n"
     ]
    }
   ],
   "source": [
    "# Read the file.\n",
    "\n",
    "with open('./data/the-verdict.txt', 'r', encoding='utf-8') as f_in:\n",
    "    raw_text = f_in.read()\n",
    "\n",
    "print(f'Num characters: {len(raw_text):,}')\n",
    "print(f'Num words: {len(raw_text.split(\" \")):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76814118-3b33-4e93-925e-c3bd643dbe0d",
   "metadata": {},
   "source": [
    "# Tokenizing text\n",
    "\n",
    "We cannot just feed raw words as input to the Transformer model. We need to first tokenize the text. Tokens are converted to embeddings which can be passed as input to the Transformer model.\n",
    "\n",
    "More specifically: `input text --> tokenized text --> token IDs --> token embeddings`\n",
    "\n",
    "Some key notes:\n",
    "- Its better to not modify the capitalization of text because it helps the LLM understand the differences between different kinds of nouns, understand sentence structure, and generate text with proper capitalization.\n",
    "- Simply splitting the text by word is not enough. We also want to separate out punctuation.\n",
    "- Whether not to remove whitespace characters is important decision to make. You probably don't want to do it in cases where the structure of the input matters. For example, python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "20a0a880-4821-407e-96fc-1fea8f5eed40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'world', '.', 'My', 'name', 'is', 'Steve', '--', '!']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def tokenize_text(text):\n",
    "    exp = r'([,.:;?_!\"()\\']|--|\\s)'\n",
    "    res = re.split(exp, text)\n",
    "    res = [x for x in res if x.strip()]\n",
    "\n",
    "    return res\n",
    "\n",
    "example_text = 'Hello world. My name is Steve--!'\n",
    "tokenize_text(example_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dc721fbc-3c04-4fb4-ba85-20e359d73713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'HAD',\n",
       " 'always',\n",
       " 'thought',\n",
       " 'Jack',\n",
       " 'Gisburn',\n",
       " 'rather',\n",
       " 'a',\n",
       " 'cheap',\n",
       " 'genius']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text = tokenize_text(raw_text)  # tokenize \n",
    "tokenized_text[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
