{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1eae78d8-7079-42f5-8f42-25896308d1ce",
   "metadata": {},
   "source": [
    "# Working with text data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de12720-b4b5-492d-8287-0fc644cc5962",
   "metadata": {},
   "source": [
    "Here we will look into:\n",
    "- Preparing text for LLM model training.\n",
    "- Splitting text into word and subword tokens.\n",
    "- Byte pair encoding.\n",
    "- Sampling training examples using a sliding window approach.\n",
    "- Converting tokens into vectors.\n",
    "\n",
    "For the purposes of learning we will work with the text of short story by Edith Wharton called \"The Verdict.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2882fea2-aff9-4e55-8e77-d6cfda0a1363",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4ddeb3e-bd87-493a-b9ea-016da894e654",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T20:37:59.313571Z",
     "start_time": "2024-06-29T20:37:59.156458Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./data/the-verdict.txt', <http.client.HTTPMessage at 0x1052cc220>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
    "       \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
    "       \"the-verdict.txt\")\n",
    "urllib.request.urlretrieve(url, './data/the-verdict.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf39fcb9-e027-4424-8d78-ac6d9b2f747e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T20:56:17.447382Z",
     "start_time": "2024-06-29T20:56:17.442675Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num characters: 20,479\n",
      "Num words: 3,552\n"
     ]
    }
   ],
   "source": [
    "# Read the file.\n",
    "\n",
    "with open('./data/the-verdict.txt', 'r', encoding='utf-8') as f_in:\n",
    "    raw_text = f_in.read()\n",
    "\n",
    "print(f'Num characters: {len(raw_text):,}')\n",
    "print(f'Num words: {len(raw_text.split(\" \")):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76814118-3b33-4e93-925e-c3bd643dbe0d",
   "metadata": {},
   "source": [
    "# Tokenizing Text\n",
    "\n",
    "We cannot just feed raw words as input to the Transformer model; we need to first tokenize the text. Tokens are then converted to embeddings, which can be passed as input to the Transformer model.\n",
    "\n",
    "More specifically: `input text --> tokenized text --> token IDs --> token embeddings`\n",
    "\n",
    "Some key notes:\n",
    "- It's better not to modify the capitalization of text because it helps the LLM understand the differences between different kinds of nouns, understand sentence structure, and generate text with proper capitalization.\n",
    "- Simply splitting the text by word is not enough; we also need to separate out punctuation.\n",
    "- Whether or not to remove whitespace characters is an important decision. You probably don't want to do it in cases where the structure of the input matters, such as in Python code.\n",
    "- Tokenizers should be designed to handle special tokens. The essential ones to consider are:\n",
    "    - End of text token\n",
    "    - Unknown token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "241b56e1-b669-4f79-9a22-980210da2348",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T20:56:29.902711Z",
     "start_time": "2024-06-29T20:56:29.899936Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class SimpleTokenizer:\n",
    "    def __init__(self, vocab: dict = None):\n",
    "        self.str_to_int = None\n",
    "        self.int_to_str = None\n",
    "        \n",
    "        if vocab:\n",
    "            self.str_to_int = vocab\n",
    "            self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "\n",
    "    @staticmethod\n",
    "    def add_special_tokens(tokens: set) -> set:\n",
    "        special_tokens = ['<|endoftext|>', '<|unk|>']\n",
    "        tokens.extend(special_tokens)\n",
    "\n",
    "        return tokens\n",
    "        \n",
    "    def build_vocab(self, text: str) -> None:\n",
    "        exp = r'([,.:;?_!\"()\\']|--|\\s)'\n",
    "        res = re.split(exp, text)\n",
    "        res = [x for x in res if x.strip()]\n",
    "        tokens = sorted(set(res))\n",
    "        tokens = self.add_special_tokens(tokens)\n",
    "        vocab = {token: i for i, token in enumerate(tokens)}\n",
    "        \n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "        \n",
    "    def encode(self, text: str) -> list[int]:\n",
    "        \"\"\"Covert text to token ids.\"\"\"\n",
    "        exp = r'([,.:;?_!\"()\\']|--|\\s)'\n",
    "        res = re.split(exp, text)\n",
    "        res = [x for x in res if x.strip()]\n",
    "        res = [token if token in self.str_to_int else '<|unk|>' for token in res]\n",
    "\n",
    "        ids = [self.str_to_int[i] for i in res]\n",
    "        \n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids: list[int]) -> str:\n",
    "        \"\"\"Convert token ids to text.\"\"\"\n",
    "        tokens = [self.int_to_str[i] for i in ids]\n",
    "        text = ' '.join(tokens)\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        \n",
    "        return text\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd906a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1131, 5, 697, 1131, 584, 1131, 0, 1130, 93, 1131, 1131, 584, 115, 508, 1131, 7, 1130, 93, 1131, 235, 1131, 1131, 741, 988, 1131, 1131, 7]\n",
      "<|unk|>, my <|unk|> is <|unk|>! <|endoftext|> The <|unk|> <|unk|> is a great <|unk|>. <|endoftext|> The <|unk|> brown <|unk|> <|unk|> over the <|unk|> <|unk|>.\n"
     ]
    }
   ],
   "source": [
    "text_1 = \"Hello, my name is Mukul!\"\n",
    "text_2 = \"The Porsche 911 is a great car.\"\n",
    "text_3 = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "all_text = \" <|endoftext|> \".join([text_1, text_2, text_3])\n",
    "\n",
    "tokenizer = SimpleTokenizer()\n",
    "tokenizer.build_vocab(raw_text)\n",
    "\n",
    "print(tokenizer.encode(all_text))\n",
    "print(tokenizer.decode(tokenizer.encode(all_text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba10533",
   "metadata": {},
   "source": [
    "# BytePair Encoding (BPE)\n",
    "\n",
    "BPE is what is used by GPT3.\n",
    "\n",
    "Key notes:\n",
    "- BPE is able to handle out of vocabulary tokens by breaking them representing them as subword tokens that is does know.\n",
    "- The BPE tokenizer has vocab size of 50,257."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ae821b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in /Users/mukul/anaconda3/envs/llm-zero-to-hero/lib/python3.10/site-packages (0.7.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/mukul/anaconda3/envs/llm-zero-to-hero/lib/python3.10/site-packages (from tiktoken) (2024.5.15)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/mukul/anaconda3/envs/llm-zero-to-hero/lib/python3.10/site-packages (from tiktoken) (2.32.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mukul/anaconda3/envs/llm-zero-to-hero/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mukul/anaconda3/envs/llm-zero-to-hero/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mukul/anaconda3/envs/llm-zero-to-hero/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mukul/anaconda3/envs/llm-zero-to-hero/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2024.6.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f185291c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.7.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "import tiktoken\n",
    "\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcaedc98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 616, 1438, 318, 31509, 377, 0, 220, 50256, 383, 28367, 16679, 318, 257, 1049, 1097, 13, 220, 50256, 383, 2068, 7586, 21831, 18045, 625, 262, 16931, 3290, 13]\n",
      "Hello, my name is Mukul! <|endoftext|> The Porsche 911 is a great car. <|endoftext|> The quick brown fox jumps over the lazy dog.\n"
     ]
    }
   ],
   "source": [
    "text_1 = \"Hello, my name is Mukul!\"\n",
    "text_2 = \"The Porsche 911 is a great car.\"\n",
    "text_3 = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "all_text = \" <|endoftext|> \".join([text_1, text_2, text_3])\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "ids = tokenizer.encode(all_text, allowed_special={\"<|endoftext|>\"})\n",
    "print(ids)\n",
    "\n",
    "decoded_text = tokenizer.decode(ids)\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88fbea2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16668, 9948, 361, 22562, 346, 396, 501, 42372, 498, 312, 32346]\n",
      "super\n",
      "cal\n",
      "if\n",
      "rag\n",
      "il\n",
      "ist\n",
      "ice\n",
      "xp\n",
      "ial\n",
      "id\n",
      "ocious\n"
     ]
    }
   ],
   "source": [
    "# Example of using BPE on an unkown word.\n",
    "\n",
    "unknown_word = \"supercalifragilisticexpialidocious\"\n",
    "\n",
    "ids = tokenizer.encode(unknown_word)\n",
    "print(ids)\n",
    "\n",
    "for id in ids:\n",
    "    print(tokenizer.decode([id]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8996a795",
   "metadata": {},
   "source": [
    "# Data Sampling\n",
    "\n",
    "LLMs learn by predicting the next word in a sequence. So, how do we convert our text data into training data? To do this, we need to slide over our data and generate {input, target} pairs, where the input is a chunk of text (tokens) and the target is the next word (token)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "736e0cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 5,145\n"
     ]
    }
   ],
   "source": [
    "with open('./data/the-verdict.txt', 'r', encoding='utf-8') as f_in:\n",
    "    raw_text = f_in.read()\n",
    "\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "enc_sample = enc_text[50:]\n",
    "print(f'Number of tokens: {len(enc_text):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e06362e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and -->  established\n",
      " and established -->  himself\n",
      " and established himself -->  in\n",
      " and established himself in -->  a\n"
     ]
    }
   ],
   "source": [
    "context_size = 4\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size]\n",
    "\n",
    "for i in range(1, context_size+1):\n",
    "    context = tokenizer.decode(enc_sample[:i])\n",
    "    target = tokenizer.decode([enc_sample[i]])\n",
    "    print(f'{context} --> {target}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061f511b",
   "metadata": {},
   "source": [
    "# Implementing a data loader class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4dc0430c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.3.1-cp310-none-macosx_11_0_arm64.whl.metadata (26 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Downloading filelock-3.15.4-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/mukul/anaconda3/envs/llm-zero-to-hero/lib/python3.10/site-packages (from torch) (4.11.0)\n",
      "Collecting sympy (from torch)\n",
      "  Downloading sympy-1.12.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Downloading networkx-3.3-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: jinja2 in /Users/mukul/anaconda3/envs/llm-zero-to-hero/lib/python3.10/site-packages (from torch) (3.1.4)\n",
      "Collecting fsspec (from torch)\n",
      "  Downloading fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/mukul/anaconda3/envs/llm-zero-to-hero/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\n",
      "Collecting mpmath<1.4.0,>=1.1.0 (from sympy->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Downloading torch-2.3.1-cp310-none-macosx_11_0_arm64.whl (61.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 MB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading filelock-3.15.4-py3-none-any.whl (16 kB)\n",
      "Downloading fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.6/177.6 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading networkx-3.3-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.12.1-py3-none-any.whl (5.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, sympy, networkx, fsspec, filelock, torch\n",
      "Successfully installed filelock-3.15.4 fsspec-2024.6.1 mpmath-1.3.0 networkx-3.3 sympy-1.12.1 torch-2.3.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b333e211",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDataset(Dataset):\n",
    "    def __init__(self, text, tokenizer, max_length, stride) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(text)\n",
    "\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.input_ids[index], self.target_ids[index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d01d6afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(text, batch_size=4, max_length=256, stride=128, shuffle=True,\n",
    "                      drop_last=True, num_workers=0):\n",
    "    tokenizer = tiktoken.get_encoding('gpt2')\n",
    "    dataset = GPTDataset(text, tokenizer, max_length, stride)\n",
    "    dataloader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf48c31b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n",
      "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n",
      "[tensor([[2885, 1464, 1807, 3619]]), tensor([[1464, 1807, 3619,  402]])]\n",
      "[tensor([[1464, 1807, 3619,  402]]), tensor([[1807, 3619,  402,  271]])]\n",
      "[tensor([[1807, 3619,  402,  271]]), tensor([[ 3619,   402,   271, 10899]])]\n"
     ]
    }
   ],
   "source": [
    "with open('./data/the-verdict.txt', 'r', encoding='utf-8') as f:\n",
    "    raw_text = f.read()\n",
    " \n",
    "dataloader = create_dataloader(\n",
    "    text=raw_text,\n",
    "    batch_size=1,\n",
    "    max_length=4,\n",
    "    stride=1,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "\n",
    "for i in range(0, 5):\n",
    "    batch = next(data_iter)\n",
    "    print(batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6e316f",
   "metadata": {},
   "source": [
    "# Creating Token Embeddings\n",
    "\n",
    "Now we need to figure out how to go from token IDs to token embeddings.\n",
    "\n",
    "Key notes:\n",
    "- We initialize the embedding weights with random values. The weights are later learned during the training process.\n",
    "- The embedding layer is essentially a lookup table that retrieves rows from the embedding matrix corresponding to the token IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a6fb7493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.9428,  0.5536,  0.6039,  0.5082, -1.1227],\n",
      "        [ 0.9949,  1.2063,  0.0420, -0.8989,  2.4404],\n",
      "        [-1.6505, -2.5095,  1.0637,  2.1797, -0.5502],\n",
      "        [ 0.2894, -0.2886,  0.6267,  0.2252, -0.6967],\n",
      "        [-0.3044,  0.2805, -0.3211, -0.7243,  0.2545],\n",
      "        [-1.0978, -0.8891, -1.3649, -0.8664,  0.6973],\n",
      "        [ 0.4946,  0.0484, -0.6427, -0.3659, -1.5711],\n",
      "        [ 0.6534, -0.8021,  0.1398, -0.7539, -0.9315],\n",
      "        [ 0.0863, -0.3487, -0.2061,  1.0613, -0.7736],\n",
      "        [ 0.3538,  0.8521,  0.2637,  1.5684, -1.3176]], requires_grad=True)\n",
      "torch.Size([10, 5])\n"
     ]
    }
   ],
   "source": [
    "# Simple example\n",
    "\n",
    "token_ids = torch.tensor([1, 5, 3, 2])\n",
    "\n",
    "vocab_size = 10\n",
    "embedding_dim = 5\n",
    "\n",
    "torch.manual_seed = 42\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "print(embedding_layer.weight)\n",
    "print(embedding_layer.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a1ced598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.9949,  1.2063,  0.0420, -0.8989,  2.4404],\n",
      "        [-1.0978, -0.8891, -1.3649, -0.8664,  0.6973],\n",
      "        [ 0.2894, -0.2886,  0.6267,  0.2252, -0.6967],\n",
      "        [-1.6505, -2.5095,  1.0637,  2.1797, -0.5502]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(token_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f635d0ee",
   "metadata": {},
   "source": [
    "# Encoding Word Positions\n",
    "\n",
    "Now we need to figure out how to convey the position of tokens in relation to each other.\n",
    "\n",
    "Key notes:\n",
    "- There are two types of positional encoding: absolute and relative. The choice of encoding depends on your use case.\n",
    "    - Absolute positional encoding:\n",
    "        - Directly associates specific positions in the input sequence with unique embeddings to convey exact locations.\n",
    "    - Relative positional encoding:\n",
    "        - Captures the relative distance between tokens, forcing the model to learn the relationship between tokens in terms of \"how far apart\" they are from one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7a0bb339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tokens, Output tokens\n",
      "torch.Size([8, 4]) torch.Size([8, 4])\n",
      "\n",
      "Token embeddings\n",
      "torch.Size([8, 4, 768])\n",
      "\n",
      "Positional embeddings\n",
      "torch.Size([4, 768])\n",
      "\n",
      "Input embeddings\n",
      "torch.Size([8, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "# Creating our embedding layer.\n",
    "\n",
    "output_dim = 768\n",
    "vocab_size = 50257\n",
    "\n",
    "token_embedding_layer = torch.nn.Embedding(\n",
    "    num_embeddings=vocab_size,\n",
    "    embedding_dim=output_dim\n",
    ")\n",
    "\n",
    "# Sample data from data loader and get token embeddings.\n",
    "data_loader = create_dataloader(\n",
    "    text=raw_text,\n",
    "    batch_size=8,\n",
    "    max_length=4,\n",
    "    stride=4,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "data_iter = iter(data_loader)\n",
    "input_tokens, output_tokens = next(data_iter)\n",
    "\n",
    "print(\"Input tokens, Output tokens\")\n",
    "print(input_tokens.shape, output_tokens.shape)\n",
    "\n",
    "token_embeddings = token_embedding_layer(input_tokens)\n",
    "\n",
    "print(\"\\nToken embeddings\")\n",
    "print(token_embeddings.shape)\n",
    "\n",
    "# Create absolute positional encoding layer.\n",
    "context_length = 4\n",
    "pos_embedding_layer = torch.nn.Embedding(\n",
    "    num_embeddings=4,\n",
    "    embedding_dim=output_dim\n",
    ")\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
    "\n",
    "print('\\nPositional embeddings')\n",
    "print(pos_embeddings.shape)\n",
    "\n",
    "# Add the positional info the token embeddings.\n",
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "\n",
    "print(\"\\nInput embeddings\")\n",
    "print(input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667a5016",
   "metadata": {},
   "source": [
    "# Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f06988d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num characters: 20,479\n",
      "Num words: 3,634\n",
      "\n",
      "Input tokens, Output tokens\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "\n",
      "Token embeddings\n",
      "torch.Size([4, 256, 768])\n",
      "\n",
      "Positional embeddings\n",
      "torch.Size([256, 768])\n",
      "\n",
      "Input embeddings\n",
      "torch.Size([4, 256, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import tiktoken\n",
    "\n",
    "\n",
    "# Load the raw data.\n",
    "def load_text(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "\n",
    "\n",
    "raw_text = load_text(\"./data/the-verdict.txt\")\n",
    "print(f\"Num characters: {len(raw_text):,}\")\n",
    "print(f\"Num words: {len(raw_text.split()):,}\")\n",
    "\n",
    "\n",
    "# Setup data loader.\n",
    "class GPTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, text, tokenizer, max_length, stride):\n",
    "        self.tokens = tokenizer.encode(text)\n",
    "        self.max_length = max_length\n",
    "        self.stride = stride\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.tokens) - self.max_length) // self.stride + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start = idx * self.stride\n",
    "        end = start + self.max_length\n",
    "        input_tokens = self.tokens[start:end]\n",
    "        output_tokens = self.tokens[start + 1 : end + 1]\n",
    "        return torch.tensor(input_tokens), torch.tensor(output_tokens)\n",
    "\n",
    "\n",
    "def create_dataloader(\n",
    "    text,\n",
    "    batch_size=4,\n",
    "    max_length=256,\n",
    "    stride=128,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=0,\n",
    "):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDataset(text, tokenizer, max_length, stride)\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "\n",
    "data_loader = create_dataloader(text=raw_text)\n",
    "data_iter = iter(data_loader)\n",
    "\n",
    "input_tokens, output_tokens = next(data_iter)\n",
    "print(\"\\nInput tokens, Output tokens\")\n",
    "print(input_tokens.shape, output_tokens.shape)\n",
    "\n",
    "\n",
    "# Token embedding\n",
    "def token_embedding_layer(input_tokens):\n",
    "    embedding_layer = torch.nn.Embedding(num_embeddings=50257, embedding_dim=768)\n",
    "    return embedding_layer(input_tokens)\n",
    "\n",
    "\n",
    "token_embeddings = token_embedding_layer(input_tokens)\n",
    "print(\"\\nToken embeddings\")\n",
    "print(token_embeddings.shape)\n",
    "\n",
    "# Positional encoding\n",
    "context_length = 256\n",
    "\n",
    "\n",
    "def create_positional_embeddings(context_length, output_dim):\n",
    "    pos_embedding_layer = torch.nn.Embedding(\n",
    "        num_embeddings=context_length, embedding_dim=output_dim\n",
    "    )\n",
    "    return pos_embedding_layer(torch.arange(context_length))\n",
    "\n",
    "\n",
    "pos_embeddings = create_positional_embeddings(\n",
    "    context_length, token_embeddings.shape[-1]\n",
    ")\n",
    "print(\"\\nPositional embeddings\")\n",
    "print(pos_embeddings.shape)\n",
    "\n",
    "# Combine embeddings\n",
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(\"\\nInput embeddings\")\n",
    "print(input_embeddings.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
